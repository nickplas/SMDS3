---
title: "Homework - Group A"
author: "Milton Nicolas Plasencia Palacios, Azad Sadr, Gaia Saveri, Alessandro Scardoni"
date: "Spring 2020"
output: 
  html_document:
    toc: yes
  beamer_presentation:
    highlight: tango
  include: null
  ioslides_presentation:
    highlight: tango
  pdf_document:
    highlight: tango
    keep_tex: yes
    toc: yes
  slide_level: 2
  slidy_presentation:
    fig.height: 3
    fig.width: 4
    highlight: tango
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Trieste
graphics: yes
subtitle: Simulation, examples and exercises
fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exercises from LEC

## Exercise 1

**Compute the bootstrap-based confidence interval for the `score` dataset using the studentized method.**

---

```{r Lec_ex1, echo=TRUE}
score <-read.table("student_score.txt", header = TRUE)

#function that return the eigenratio statistic
psi_fun <-function(data) {
        eig <-eigen(cor(data))$values
        return(max(eig)/ sum(eig))}

#observed value
psi_obs <-psi_fun(score)

#Computing standard error
n <-nrow(score); B <- 10^4
s_vect <-rep(0, B)
SE_jack <-rep(0,B)
s1_vect<-rep(0,n)
for(i in 1:B){
  ind <- sample(1:n, n, replace = TRUE)
  s_vect[i] <- psi_fun(score[ind,])
  for(j in 1:n) s1_vect[j] <- psi_fun(score[ind,][-j,])
  SE_jack[i] <- sqrt(((n - 1)/n) * sum((s1_vect - mean(s1_vect))^2))
}
SE_boot <-sd(s_vect)

z<- (s_vect - psi_obs)/SE_jack

#Wald interval
wald_ci <- psi_obs + c(-1, 1)*1.96*SE_boot 
wald_ci
```

```{r Lec_ex1.1, echo=TRUE}

#Percentile method
perc_ci <-quantile(s_vect, prob=c(0.025, 0.975))
attr(perc_ci, "names") <- NULL
perc_ci

```

```{r Lec_ex1.2, echo=TRUE}
#Confidence Interval
studentized_ci <- psi_obs - SE_boot*quantile(z, prob=c(0.975, 0.025))
studentized_ci

```
As we can see the Confidence Interval of the studentized method is wider than the other ones.

```{r Lec_ex1.3, echo=TRUE}
library(MASS)
hist.scott(s_vect, main = "")
abline(v = psi_obs, col = 2)
abline(v = studentized_ci, col = 4)
```

## Exercise 2

**Compute bootstrap-based confidence intervals for the `score` dataset using the `boot` package.**

---

```{r Lec_ex2, echo=TRUE}
library(boot)
score <-read.table("student_score.txt", header = TRUE)

boot1_fun <-function(data, i) {
  a<- data[i,]
  eig <- eigen(cor(a))$values
  return(max(eig)/ sum(eig))
}
boot1 <- boot(data = score, statistic = boot1_fun, R = 100)
boot_psi <- boot(data = score, statistic = boot1_fun, R = 10^4)
boot.ci(boot_psi, type = "perc")

```

```{r Lex_ex2.1, echo=TRUE}
psi_fun_var <-function(data, i){
    a<- data[i,]
    eig <- eigen(cor(a))$values
    boot1 <- boot(data = score, statistic = boot1_fun, R= 50 )
    return(c(max(eig)/ sum(eig), var(boot1$t)))
}

results <- boot(data = score, statistic = psi_fun_var, R=100)
boot.ci(results, type = "stud" )
```

# Lab exercises

## Exercise 1

**Use `nlm` to compute the variance for the estimator $\hat w = (\log(\hat{\gamma}),\log(\hat{\beta}))$ and `optimHess` for the variance of $\hat{\theta}=(\hat{\gamma}, \hat{\beta})$.**

---

```{r, warning=FALSE}
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146, 170.3, 148, 140, 118, 144, 97)
n <- length(y)
log_lik_weibull <- function( data, param) {
    -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
```

parameters estimates will be expresseed in the log-scale usin `theta` function, and we need to re-express them in the original scale using `omega` function.

```{r, warning=FALSE}
omega <- function(theta) log(theta)
theta <- function(omega) exp(omega)
log_lik_weibull_rep <- function(data, param) log_lik_weibull(data, theta(param))
weib.y.nlm <- nlm(log_lik_weibull_rep, c(0,0), hessian = T, data = y)
weib.y.nlm
```

estimated variance of the MLE are the diagonal elements of the observed Fisher information matrix.
so variance for the estimator $\hat w = (\log(\hat{\gamma}),\log(\hat{\beta}))$ usin `nlm`are:

```{r, warning=FALSE}
diag(solve(weib.y.nlm$hessian))
```


```{r, warning=FALSE}
optimhess <- optimHess(theta(weib.y.nlm$estimate),log_lik_weibull,data=y)
optimhess 
```

variance of $\hat{\theta}=(\hat{\gamma}, \hat{\beta})$ using `optimHess` are
```{r, warning=FALSE}
diag(solve(optimhess))
```


## Exercise 2

**The Wald confidence interval with level $1 - \alpha$ is defined as:**

$$\hat \gamma \pm z_{1−\alpha/2}j_P(\hat \gamma)^{−1/2}$$

**Compute the Wald confidence interval of level 0.95 and plot the results.**

---

```{r echo=TRUE,  message=FALSE, warning=FALSE}
#log-likelihood profile function
log_lik_weibull_profile_gamma  <- function(data, gamma){
  beta.gamma <- mean(data^gamma)^(1/gamma)
  log_lik_weibull( data, c(gamma, beta.gamma) )
}
log_lik_weibull_profile_gamma_v <-Vectorize(log_lik_weibull_profile_gamma, 'gamma'  )
conf.level<-0.95
# MLE estimation using log likelihood profile with beta as nuisance parameter.
weib.y.mle<-optim(1 ,fn=log_lik_weibull_profile_gamma,hessian=T,
                  method='L-BFGS-B',lower=rep(1e-7,2),
                  upper=rep(Inf,2),data=y)
#Wald CI
# calculate standard error using the Hessian Matrix
weib.y.se<-sqrt(diag(solve(weib.y.mle$hessian)))
# calculate the two extremes of Wald CI.
wald.ci1<-weib.y.mle$par[1]+c(-1,1)*qnorm(1-(1-conf.level)/2)*weib.y.se[1]
wald.ci1
#Plot values for Profile ML with beta as nuisance parameter.
plot(function(x) -log_lik_weibull_profile_gamma_v(data=y, x)+weib.y.mle$value,
     from=0.1,to=15,xlab=expression(gamma),
     ylab='profile relative log likelihood',ylim=c(-8,0))
segments( wald.ci1[1], -log_lik_weibull_profile_gamma_v(y,wald.ci1[1])-weib.y.mle$value,
          wald.ci1[1], -log_lik_weibull_profile_gamma_v(y, wald.ci1[1])+weib.y.mle$value, 
          col="blue", lty=2)
segments( wald.ci1[2],
          -log_lik_weibull_profile_gamma_v(y,wald.ci1[2])-weib.y.mle$value, wald.ci1[2], -log_lik_weibull_profile_gamma_v(y, wald.ci1[2])+weib.y.mle$value, 
          col="blue", lty=2 )
segments(  wald.ci1[1], -7,  wald.ci1[2], -7, col="blue", lty =1, lwd=2)
points(wald.ci1[1], -qchisq(0.95,1)/2, pch=16, col="blue", cex=1.5)
points(wald.ci1[2], -qchisq(0.95,1)/2, pch=16, col="blue", cex=1.5)
abline(h=-qchisq(conf.level,1)/2,lty='dashed',col="blue")
abline(v=weib.y.mle$par[1], col=5, lwd=1, lty=2)
text(7,-6,"95% Wald CI",col="blue")
```

## Exercise 3

**Repeat the steps above - write the profile log-likelihood, plot it and find the deviance confidence intervals - considering this time $\gamma$ as a nuisance parameter and $\beta$ as the parameter of interest.**

---

We fix $\gamma = \hat{\gamma}$, namely the root of:

$$
\frac{\partial \ell(\gamma,\beta;y)}{\partial\gamma} = \frac{n}{\gamma} - n\log(\beta) + \sum_{i=1}^{n}\log(y_i) - \sum_{i=1}^{n}(\frac{y_i}{\beta})^{\gamma}\log(\frac{y_i}{\beta}) = 0
$$

since it is not possible to express $\gamma$ as function of $\beta$ in a closed form. 

```{r Lab_ex3, echo = TRUE}
y<-c(155.9, 200.2, 143.8, 150.1, 152.1, 142.2, 147, 146, 146, 
     170.3, 148, 118, 144, 97)

n<-length(y)

log_lik_weibull<-function(data, param) {
  -sum(dweibull(data, shape=param[1], scale=param[2], log=TRUE))
}

weib.y.mle<-optim(c(1,1),, fn=log_lik_weibull, hessian=T,
                  method='L-BFGS-B', lower=rep(1e-7,2),
                  upper=rep(Inf,2),data=y)

log_lik_weibull_profile<-function(data, beta) {
  gamma.beta<-uniroot(function(x)n/x+sum(log(y))-n*sum(y^x*log(y))/sum(y^x),
                      c(1e-5,15))$root
  log_lik_weibull(data, c(gamma.beta, beta))
}

log_lik_weibull_profile_v<-Vectorize(log_lik_weibull_profile, 'beta')

plot(function(x) -log_lik_weibull_profile_v(data=y, x)+weib.y.mle$value,
     from=120, to=200, xlab=expression(beta),
     ylab='profile relative log likelihood', ylim=c(-10,0))

conf.level<-0.95
abline(h=-qchisq(conf.level,1)/2,lty='dashed',col=2)

lrt.ci1<-uniroot(function(x) 
                 -log_lik_weibull_profile_v(y, x)
                 +weib.y.mle$value
                 +qchisq(conf.level,1)/2,
                  c(1e+7,weib.y.mle$par[2]))$root

lrt.ci1<-c(lrt.ci1,uniroot(function(x) 
                           -log_lik_weibull_profile_v(y,x)
                           +weib.y.mle$value
                           +qchisq(conf.level,1)/2,
                           c(weib.y.mle$par[2],15))$root)

segments( lrt.ci1[1],-qchisq(conf.level,1)/2, lrt.ci1[1],
          -log_lik_weibull_profile_v(y, lrt.ci1[1]), col="red", lty=2)

segments( lrt.ci1[2],-qchisq(conf.level,1)/2, lrt.ci1[2],
          -log_lik_weibull_profile_v(y, lrt.ci1[2]), col="red", lty=2)

points(lrt.ci1[1], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)
points(lrt.ci1[2], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)
segments( lrt.ci1[1], -10, lrt.ci1[2],-10, col="red", lty =1, lwd=2)
text(158,-9.6,"95% Deviance CI",col=2, cex=0.7)
```

## Exercise 5

**In `sim` in the code above, you find the MCMC output which allows to approximate the posterior distribution of our parameter of interest with $\textbf S$ draws of $\theta$. Please produce an histogram for these random draws $\theta^{(1)},\dots,\theta^{(S)}$, compute the empirical quantiles, and overlap the true posterior distribution.**

---

```{r Lab_ex5, echo=TRUE}
library(rstan)
set.seed(42)
#retrieve sim

model_code<-"
data{
  int N;
  real y[N];
  real<lower=0> sigma;
  real mu;
  real<lower=0> tau;
}
parameters{
  real theta;
}
model{
  target+=normal_lpdf(y|theta, sigma);
  target+=normal_lpdf(theta|mu, tau );
}
"

n<-10 #sample size 
sigma2<-2 #likelihood variance
mu<-7 #prior mean
tau2<-2 #prior variance
theta_sample<-2 #true mean
y<-rnorm(n,theta_sample,sqrt(sigma2))

data<-list(N=n, y=y, sigma=sqrt(sigma2), mu=mu, tau=sqrt(tau2))
fit<-stan(model_code = model_code, data=data, chains=4, iter=2000)

sim <- extract(fit) #mcmc output

#posterior mean
mu_star <- ((1/tau2)*mu+(n/sigma2)*mean(y))/((1/tau2)+(n/sigma2))
#posterior standard deviation
sd_star <- sqrt(1/((1/tau2)+(n/sigma2)))

#stan posterior
hist(sim$theta, breaks=40, xlab=expression(theta), ylab="", main="", probability=TRUE)
#true posterior
curve(dnorm(x, mu_star, sd_star), 
       col="blue", lwd=2, add=T)
#empirical quantiles
q<-quantile(sim$theta)
segments(q,0,q, dnorm(q, mu_star, sd_star), col=2)

legend(3.7, 1, c("Stan posterior", "True posterior", "Empirical quantiles"), 
  c("black", "blue", "red"), lty=c(2,1,1),lwd=c(1,1,1), cex=0.5)
```

## Exercise 6

**Launch the following line of R code:**

```{r Lab6, echo=TRUE}
posterior<-as.array(fit)
```

**Use now the `bayesplot` package. Read the help and produce for this example, using the object `posterior`, the following plots:**

* **posterior intervals.**
* **posterior areas.**
* **marginal posterior distributions for the parameters.**

**Quickly comment.**

---

The object `posterior` is a 3-D array of MCMC draws, for both the parameters `theta` and `lp__`, as observed from the output of `dimnames(posterior)`. We are interested only in `theta`.  

```{r Lab_ex6_int, echo=TRUE}
library(bayesplot)
#posterior intervals
color_scheme_set("brightblue")
mcmc_intervals(posterior, pars=c("theta"))
```

As reported in the `bayesplot` documentation, this is the plot of uncertainty intervals computed from posterior draws with all chains merged, based on quantiles. The thick segment represent the 50% interval, while the thinner outer lines represent the 90% interval. The circle is the posterior median. Since we know the true value of $\theta$ (`mu_star` ≈ 3.158), we can observe that it falls inside the 90% uncertainty interval. 

```{r Lab_ex6_area, warning = FALSE, echo=TRUE}
#posterior areas
color_scheme_set("teal")
mcmc_areas(posterior, pars = c("theta"), prob=0.8, prob_outer = 0.99, point_est="mean") + ggplot2::labs(
  title = "Posterior distribution",
  subtitle = "with mean and 80% intervals"
) 
```

The function `mcmc_areas` shows the uncertainty intervals as shaded areas under the estimated posterior density curve. Here again we can observe that the true value for $\theta$ falls inside the 80% interval.

```{r Lab_ex6_marginal, echo=TRUE, wrning=FALSE}
#marginal posterior distribution for the parameters
color_scheme_set("viridis")
mcmc_hist(posterior, pars=c("theta"), binwidth=0.05)
```

The function `mcmc_hist` plots the marginal posterior distribution of the parameter of interest, namely $\theta$, combining all the chains.

## Ecercise 7

**Suppose you receive n=15 phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $ y_i \sim Po(\lambda) $. Now, you have to choose the prior $\pi (\lambda) $. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:**

**1) $\pi (\lambda)$ = BETA(4,2) **

**2) $\pi (\lambda)$ = $N$(1,2) **

**3) $\pi (\lambda)$ = Gamma(4,2) **

**Now, compute your posterior as $\pi (\lambda|y) \propto L(\lambda;y)\pi(\lambda)$ for the selected prior. If your first choice was correct, you will be able to compute it analitically.**

I decide to use the Gamma distribution as a prior because it is defined in the interval $[0, +\infty]$an it is also the conjugate prior of the Poisson distribution. Moreover we can observe that:

1) The beta distribution is excluded because its support is $[0,1]$ so it can not be used to assess the average lengh of a phone call, which can take every value in $\mathbb{R}^+$.

2) The normal distribution is excluded because it can take also negative values, which is not consistent with our problem.

Analitical computation:
$$ \pi(\lambda|y) \propto L(\lambda;y)\pi(\lambda) = \left[ \prod_{i=1}^{15} \frac{\lambda ^{y_i} e^{-\lambda}}{y_i!} \right] \left[\frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha - 1} e^{-\beta\lambda}\right] $$
and so:

$$\pi(\lambda|y) \propto \lambda^{\sum y_i +\alpha -1 } e^{-(n+\beta)\lambda}$$

which is proportional to a Gamma distribution. So:

$$\pi(\lambda|y) \sim Gamma \left( \sum_{i=1}^{15} y_i + \alpha, 15+\beta \right) $$ 

with $\alpha = 4$ and $\beta = 2$.



## Exercise 9

```{r Lab_ex9, include=TRUE}
knitr::opts_chunk$set
library(LearnBayes)
data(soccergoals)

y <- soccergoals$goals

#write the likelihood function via the gamma distribution


lik_pois<- function(data, theta){
  n <- length(data)
  lambda <- exp(theta)
  dgamma(lambda, shape =sum(data)+1, scale=1/n)
}

prior_gamma <- function(par, theta){
  lambda <- exp(theta)
  dgamma(lambda, par[1], rate=par[2])*lambda  
}

prior_norm <- function(npar, theta){
  lambda=exp(theta)  
  (dnorm(theta, npar[1], npar[2]))
  
}

lik_pois_v <- Vectorize(lik_pois, "theta")
prior_gamma_v <- Vectorize(prior_gamma, "theta")
prior_norm_v <- Vectorize(prior_norm, "theta")


#likelihood
curve(lik_pois_v(theta=x, data=y), xlim=c(-1,4), xlab=expression(theta), ylab = "density", lwd =2 )
#prior 1
curve(prior_gamma_v(theta=x, par=c(2, 4)), lty =2, col="red", add = TRUE, lwd =2)
#prior 2 
curve(prior_norm_v(theta=x, npar=c(1, .5)), lty =3, col="blue", add =TRUE, lwd=2)
#prior 3 
curve(prior_norm_v(theta=x, npar=c(2, .5)), lty =4, col="green", add =TRUE, lwd =2)
#prior 4 
curve(prior_norm_v(theta=x, npar=c(1, 2)), lty =5, col="violet", add =TRUE, lwd =2)
legend(2.6, 1.8, c("Lik.", "Ga(2,4)", "N(1, 0.25)", "N(2,0.25)","N(1, 4)" ),
       lty=c(1,2,3,4,5), col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)

logpoissongamma <- function(theta, datapar){
  data <- datapar$data
  par <- datapar$par
  lambda <- exp(theta)
  log_lik <- log(lik_pois(data, theta))
  log_prior <- log(prior_gamma(par, theta))
  return(log_lik+log_prior)
}

logpoissongamma.v <- Vectorize( logpoissongamma, "theta")


logpoissonnormal <- function( theta, datapar){
  data <- datapar$data
  npar <- datapar$par
  lambda <- exp(theta)
  log_lik <- log(lik_pois(data, theta))
  log_prior <- log(prior_norm(npar, theta))
  return(log_lik+log_prior)
}  
logpoissonnormal.v <- Vectorize( logpoissonnormal, "theta")

#log-likelihood
curve(log(lik_pois(y, theta=x)), xlim=c(-1,4),ylim=c(-20,2), lty =1,
      ylab="log-posteriors", xlab=expression(theta))
#log posterior 1
curve(logpoissongamma.v(theta=x, list(data=y, par=c(2, 4))), col="red", xlim=c(-1,4),ylim=c(-20,2), lty =1, add =TRUE)
#log posterior 2
curve(logpoissonnormal.v( theta=x, datapar <- list(data=y, par=c(1, .5))), lty =1, col="blue",  add =TRUE)
#log posterior 3
curve(logpoissonnormal.v( theta=x, datapar <- list(data=y, par=c(2, .5))), lty =1, col="green", add =TRUE, lwd =2)
#log posterior 4
curve(logpoissonnormal.v( theta=x, list(data=y, par=c(1, 2))), lty =1, col="violet", add =TRUE, lwd =2)
legend(2.6, 1.3, c( "loglik", "lpost 1", "lpost 2", "lpost 3", "lpost 4" ),
       lty=1, col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)

datapar <- list(data=y, par=c(2, 4))
fit1 <- laplace(logpoissongamma, .5, datapar)
datapar <- list(data=y, par=c(1, .5))
fit2 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(2, .5))
fit3 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(1, 2))
fit4 <- laplace(logpoissonnormal, .5, datapar)

postmode <- c(fit1$mode, fit2$mode, fit3$mode, fit4$mode )
postsds <- sqrt(c(fit1$var, fit2$var, fit3$var, fit4$var))
logmarg <- c(fit1$int, fit2$int, fit3$int, fit4$int)
cbind(postmode, postsds, logmarg)

BF_matrix <- matrix(1, 4,4)
for (i in 1:3){
  for (j in 2:4){
    BF_matrix[i,j]<- exp(logmarg[i]-logmarg[j])
    BF_matrix[j,i]=(1/BF_matrix[i,j]) 
  }
}

round_bf <- round(BF_matrix,3)
round_bf


```

As we can see by the table, also after having changed the prior1, the prior2 remains the favourite over all. More, the bayes factor of p2/p1 has grown from 1.2 to 6.3, indicating that the new prior1~Gamma(2,4), that assume then a lambda of 1/2 is probably a worst assumption compared to the previous distribution. We could see this also by the plot1, when we see that the new distribution assigned to the prior1 pushes her away from the likelihood if compared with the plot when prior1~Gamma(4.57,1.43).

